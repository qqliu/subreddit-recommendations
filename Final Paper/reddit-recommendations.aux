\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Data}{1}{section.2}}
\newlabel{sec:data}{{2}{1}{Data}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Data Collection}{1}{subsection.2.1}}
\newlabel{sec:feature-mapping}{{2.1}{1}{Data Collection}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Number of Posts}{1}{section*.2}}
\@writefile{toc}{\contentsline {paragraph}{Number of Comments}{2}{section*.3}}
\@writefile{toc}{\contentsline {paragraph}{Length of Comments}{2}{section*.4}}
\@writefile{toc}{\contentsline {paragraph}{Post Scores}{2}{section*.5}}
\@writefile{toc}{\contentsline {paragraph}{Shared Users}{2}{section*.6}}
\@writefile{toc}{\contentsline {paragraph}{Text Similarity}{2}{section*.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Data Preprocessing}{2}{subsection.2.2}}
\newlabel{sec:data-prepros}{{2.2}{2}{Data Preprocessing}{subsection.2.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Gaussian Normalization}{2}{section*.8}}
\@writefile{toc}{\contentsline {paragraph}{Log Normalization}{3}{section*.9}}
\@writefile{toc}{\contentsline {paragraph}{\IeC {\textquotedblleft }TF-IDF-like\IeC {\textquotedblright } Weighting}{3}{section*.10}}
\@writefile{toc}{\contentsline {paragraph}{Segmentation}{3}{section*.11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Text processing}{3}{subsection.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Recommender Architecture}{4}{section.3}}
\newlabel{sec:rec-arch}{{3}{4}{Recommender Architecture}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Similarity Measures}{4}{subsection.3.1}}
\@writefile{toc}{\contentsline {paragraph}{Pearson Correlation}{4}{section*.12}}
\@writefile{toc}{\contentsline {paragraph}{Euclidean Distance}{5}{section*.13}}
\@writefile{toc}{\contentsline {paragraph}{Spearman Correlation}{5}{section*.14}}
\@writefile{toc}{\contentsline {paragraph}{Log Likelihood}{5}{section*.15}}
\@writefile{toc}{\contentsline {paragraph}{Tanimoto Coefficient}{5}{section*.16}}
\@writefile{toc}{\contentsline {paragraph}{Uncentered Cosine}{6}{section*.17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}User Neighborhood Algorithms}{6}{subsection.3.2}}
\@writefile{toc}{\contentsline {paragraph}{Nearest N Neighbors}{6}{section*.18}}
\@writefile{toc}{\contentsline {paragraph}{Threshold-based neighborhood}{6}{section*.19}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{6}{section.4}}
\newlabel{sec:results}{{4}{6}{Results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Preference Values and Feature Mappings}{6}{subsection.4.1}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Details about each file. We compute the mean and standard deviation of the mean of each user vector, the mean difference between each user feature value and the mean of that user's feature values, and the root mean square of the different between each user feature value and the mean of the user's feature values. Each file is normalized by dividing by the maximum feature value for each user.\relax }}{7}{table.caption.20}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{dataset-chars}{{1}{7}{Details about each file. We compute the mean and standard deviation of the mean of each user vector, the mean difference between each user feature value and the mean of that user's feature values, and the root mean square of the different between each user feature value and the mean of the user's feature values. Each file is normalized by dividing by the maximum feature value for each user.\relax }{table.caption.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Feature Mapping and Recommendation Quantitative Results}{7}{subsection.4.2}}
\newlabel{sec:feature-mapping}{{4.2}{7}{Feature Mapping and Recommendation Quantitative Results}{subsection.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Precision and recall due to different similarity measures that resulted in smallest root-mean-squared error on different feature maps. We see that the Pearson correlation similarity measure on the number of posts data file performs better than other similarity measures for other files in terms of precision and performs not much worse in terms of recall.\relax }}{8}{figure.caption.21}}
\newlabel{fig:feature-map}{{1}{8}{Precision and recall due to different similarity measures that resulted in smallest root-mean-squared error on different feature maps. We see that the Pearson correlation similarity measure on the number of posts data file performs better than other similarity measures for other files in terms of precision and performs not much worse in terms of recall.\relax }{figure.caption.21}{}}
\newlabel{fig:nvp}{{\caption@xref {fig:nvp}{ on input line 390}}{8}{Feature Mapping and Recommendation Quantitative Results}{figure.caption.22}{}}
\newlabel{sub@fig:nvp}{{}{8}{Feature Mapping and Recommendation Quantitative Results}{figure.caption.22}{}}
\newlabel{fig:tvp}{{\caption@xref {fig:tvp}{ on input line 395}}{8}{Feature Mapping and Recommendation Quantitative Results}{figure.caption.22}{}}
\newlabel{sub@fig:tvp}{{}{8}{Feature Mapping and Recommendation Quantitative Results}{figure.caption.22}{}}
\newlabel{fig:nvr}{{\caption@xref {fig:nvr}{ on input line 400}}{8}{Feature Mapping and Recommendation Quantitative Results}{figure.caption.22}{}}
\newlabel{sub@fig:nvr}{{}{8}{Feature Mapping and Recommendation Quantitative Results}{figure.caption.22}{}}
\newlabel{fig:tvr}{{\caption@xref {fig:tvr}{ on input line 405}}{8}{Feature Mapping and Recommendation Quantitative Results}{figure.caption.22}{}}
\newlabel{sub@fig:tvr}{{}{8}{Feature Mapping and Recommendation Quantitative Results}{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Precision and recall due to different similarity measures: Pearson correlation and Tanimoto coefficient on the file mapping user preferences to the number of posts made in a subreddit. We vary the number of nearest neighbors and the threshold similarity value when computing the neighborhood of similar users.\relax }}{8}{figure.caption.22}}
\newlabel{fig:learning-rate-hidden-layers}{{2}{8}{Precision and recall due to different similarity measures: Pearson correlation and Tanimoto coefficient on the file mapping user preferences to the number of posts made in a subreddit. We vary the number of nearest neighbors and the threshold similarity value when computing the neighborhood of similar users.\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Different Normalization Results}{8}{subsection.4.3}}
\newlabel{sec:different-normalization}{{4.3}{8}{Different Normalization Results}{subsection.4.3}{}}
\newlabel{fig:norm-n10}{{\caption@xref {fig:norm-n10}{ on input line 435}}{9}{Different Normalization Results}{figure.caption.23}{}}
\newlabel{sub@fig:norm-n10}{{}{9}{Different Normalization Results}{figure.caption.23}{}}
\newlabel{fig:norm-n50}{{\caption@xref {fig:norm-n50}{ on input line 440}}{9}{Different Normalization Results}{figure.caption.23}{}}
\newlabel{sub@fig:norm-n50}{{}{9}{Different Normalization Results}{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Precision and recall due to different normalization methods on the datasets: NP and NC (number of posts and number of comments).\relax }}{9}{figure.caption.23}}
\newlabel{fig:tfidf-p}{{3}{9}{Precision and recall due to different normalization methods on the datasets: NP and NC (number of posts and number of comments).\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Segmentation of Comments and Posts}{9}{subsection.4.4}}
\newlabel{sec:set-com-posts}{{4.4}{9}{Segmentation of Comments and Posts}{subsection.4.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Users who commented or posted to $2$ or $3$ subreddits}{9}{subsubsection.4.4.1}}
\newlabel{2-3-users}{{4.4.1}{9}{Users who commented or posted to $2$ or $3$ subreddits}{subsubsection.4.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Users who commented or posted to $3$ to $6$ subreddits}{9}{subsubsection.4.4.2}}
\newlabel{3-6-users}{{4.4.2}{9}{Users who commented or posted to $3$ to $6$ subreddits}{subsubsection.4.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.3}Users who commented or posted to more than $25$ subreddits}{10}{subsubsection.4.4.3}}
\newlabel{more-than-25}{{4.4.3}{10}{Users who commented or posted to more than $25$ subreddits}{subsubsection.4.4.3}{}}
\newlabel{fig:2-3-p}{{\caption@xref {fig:2-3-p}{ on input line 492}}{10}{Users who commented or posted to more than $25$ subreddits}{figure.caption.24}{}}
\newlabel{sub@fig:2-3-p}{{}{10}{Users who commented or posted to more than $25$ subreddits}{figure.caption.24}{}}
\newlabel{fig:3-6-p}{{\caption@xref {fig:3-6-p}{ on input line 497}}{10}{Users who commented or posted to more than $25$ subreddits}{figure.caption.24}{}}
\newlabel{sub@fig:3-6-p}{{}{10}{Users who commented or posted to more than $25$ subreddits}{figure.caption.24}{}}
\newlabel{fig:6-15-p}{{\caption@xref {fig:6-15-p}{ on input line 502}}{10}{Users who commented or posted to more than $25$ subreddits}{figure.caption.24}{}}
\newlabel{sub@fig:6-15-p}{{}{10}{Users who commented or posted to more than $25$ subreddits}{figure.caption.24}{}}
\newlabel{fig:2-3}{{\caption@xref {fig:2-3}{ on input line 507}}{10}{Users who commented or posted to more than $25$ subreddits}{figure.caption.24}{}}
\newlabel{sub@fig:2-3}{{}{10}{Users who commented or posted to more than $25$ subreddits}{figure.caption.24}{}}
\newlabel{fig:3-6}{{\caption@xref {fig:3-6}{ on input line 512}}{10}{Users who commented or posted to more than $25$ subreddits}{figure.caption.24}{}}
\newlabel{sub@fig:3-6}{{}{10}{Users who commented or posted to more than $25$ subreddits}{figure.caption.24}{}}
\newlabel{fig:2-3}{{\caption@xref {fig:2-3}{ on input line 517}}{10}{Users who commented or posted to more than $25$ subreddits}{figure.caption.24}{}}
\newlabel{sub@fig:2-3}{{}{10}{Users who commented or posted to more than $25$ subreddits}{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Precision and recall due to different similarity measures and different segmentation.\relax }}{10}{figure.caption.24}}
\newlabel{fig:learning-and-segments}{{4}{10}{Precision and recall due to different similarity measures and different segmentation.\relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.4}Users who commented or posted to $6$ to $15$ subreddits}{10}{subsubsection.4.4.4}}
\newlabel{6-15-users}{{4.4.4}{10}{Users who commented or posted to $6$ to $15$ subreddits}{subsubsection.4.4.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.5}Users who commented or posted to $15$ to $1000$ subreddits}{10}{subsubsection.4.4.5}}
\newlabel{15-1000-users}{{4.4.5}{10}{Users who commented or posted to $15$ to $1000$ subreddits}{subsubsection.4.4.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Next Steps}{10}{section.5}}
\newlabel{sec:qualitative-evaluations}{{5}{10}{Next Steps}{section.5}{}}
\gdef \LT@i {\LT@entry 
    {1}{60.36958pt}\LT@entry 
    {1}{66.06006pt}\LT@entry 
    {1}{54.67912pt}\LT@entry 
    {1}{54.67912pt}\LT@entry 
    {1}{33.33955pt}\LT@entry 
    {1}{33.33955pt}\LT@entry 
    {1}{33.33955pt}\LT@entry 
    {1}{33.33955pt}\LT@entry 
    {1}{54.67912pt}\LT@entry 
    {1}{54.67912pt}}
\bibdata{ref}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Partial table of training data including all data for the various feature mapping techniques. Data for segmentation and Gaussian normalization is not included in this table because entering such data into the table manually would take approximately 2 hours. Generalized trends from the data are shown in the sections in the body of the paper. Training Percentage is $0.9$ and $0.01$ is tested. $N$ is the number of nearest neighbors. $T$ is the threshold used in the Threshold algorithms. $H$ is the $H$-th highest rating items taken out from the set when calculating precision and recall. RMS Err is the root-mean-square error. Precision and recall are as defined above.}}{19}{table.2}}
\newlabel{results-basic}{{2}{19}{Partial table of training data including all data for the various feature mapping techniques. Data for segmentation and Gaussian normalization is not included in this table because entering such data into the table manually would take approximately 2 hours. Generalized trends from the data are shown in the sections in the body of the paper. Training Percentage is $0.9$ and $0.01$ is tested. $N$ is the number of nearest neighbors. $T$ is the threshold used in the Threshold algorithms. $H$ is the $H$-th highest rating items taken out from the set when calculating precision and recall. RMS Err is the root-mean-square error. Precision and recall are as defined above}{table.2}{}}
